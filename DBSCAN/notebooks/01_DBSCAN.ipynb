{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afe503a6-55bf-4bd6-948d-3b8604251cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimaler Epsilon-Wert: 15.074167823791504\n",
      "Länge von cluster_labels: 45000\n",
      "Anzahl der Cluster: 450\n",
      "Silhouetten-Koeffizient für die gemergten Cluster: 0.3010570165284466\n",
      "Anzahl der neuen zusammengeführten Cluster: 14\n",
      "Gemergte Cluster-Daten gespeichert!\n",
      "Gemergte Cluster-Daten als .npy gespeichert!\n",
      "Davies-Bouldin-Index (gemergte Cluster): 7.587476521937715\n",
      "Das DBSCAN-Modell wurde gespeichert!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import load_npz  # Zum Laden der speicheroptimierten Distanzmatrix\n",
    "from sklearn.cluster import DBSCAN  # DBSCAN-Algorithmus\n",
    "from sklearn.metrics import davies_bouldin_score  # Davies-Bouldin-Index zur Clusterbewertung\n",
    "from sklearn.metrics import silhouette_score  # Silhouette-Score zur Clusterbewertung\n",
    "from scipy.spatial.distance import cdist  # Zur Berechnung der Distanzen zwischen den Clusterzentren\n",
    "import joblib  # Zum Speichern des Modells\n",
    "\n",
    "# Die ersten 45.000 Zeilen der Daten laden\n",
    "df_encoded = pd.read_csv(\"dbscan_encoded_data.csv\").iloc[:45000]\n",
    "\n",
    "num_samples = len(df_encoded)  # Anzahl der Samples\n",
    "batch_size = 100  # Batch-Größe für die Verarbeitung\n",
    "num_batches = (num_samples // batch_size) + 1  # Berechne die Anzahl der Batches\n",
    "\n",
    "# ------------------------------------\n",
    "# Verbesserte Epsilon-Bestimmung\n",
    "# ------------------------------------\n",
    "k = 10  # Anzahl der nächsten Nachbarn zur Bestimmung von Epsilon\n",
    "distance_samples = []  # Liste zur Speicherung der Distanzen der k-nächsten Nachbarn\n",
    "\n",
    "# Ziehe Stichproben aus mehreren Batches\n",
    "for batch in range(num_batches):\n",
    "    try:\n",
    "        # Lade die Distanzmatrix für den aktuellen Batch\n",
    "        batch_matrix = load_npz(f\"combined_distance_batch_{batch}.npz\").toarray()\n",
    "        batch_matrix = batch_matrix[:batch_size, :batch_size]  # Sicherstellen, dass die Matrix quadratisch ist\n",
    "        \n",
    "        # Zufällige Stichprobe von Punkten im aktuellen Batch ziehen\n",
    "        num_sample_points = min(100, batch_matrix.shape[0])  # Maximal 100 Punkte pro Batch\n",
    "        sample_indices = np.random.choice(batch_matrix.shape[0], num_sample_points, replace=False)\n",
    "        \n",
    "        # Hole die Distanzen der k-nächsten Nachbarn\n",
    "        sample_values = batch_matrix[sample_indices, k - 1]  # k-Nächster Nachbar\n",
    "        distance_samples.extend(sample_values)  # Füge die Distanzen zur Liste hinzu\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "\n",
    "# Bestimme das 95%-Quantil der Distanzen als optimalen Epsilon-Wert\n",
    "epsilon = np.percentile(distance_samples, 95)\n",
    "print(f\"Optimaler Epsilon-Wert: {epsilon}\")\n",
    "\n",
    "# --- DBSCAN schrittweise auf Batches anwenden ---\n",
    "cluster_labels = np.full(num_samples, -1, dtype=int)  # Alle Punkte initial als Noise (-1)\n",
    "\n",
    "cluster_id = 0  # Start-ID für die Cluster\n",
    "for batch in range(num_batches):\n",
    "    try:\n",
    "        # Lade die Distanzmatrix für den aktuellen Batch\n",
    "        batch_matrix = load_npz(f\"combined_distance_batch_{batch}.npz\").toarray()\n",
    "        batch_matrix = batch_matrix[:batch_size, :batch_size]  # Sicherstellen, dass die Matrix quadratisch ist\n",
    "        \n",
    "        # Wende DBSCAN auf diesen Batch an\n",
    "        dbscan = DBSCAN(eps=epsilon, min_samples=30, metric=\"precomputed\")  # Epsilon und min_samples definieren den Cluster\n",
    "        batch_labels = dbscan.fit_predict(batch_matrix)  # Cluster für den Batch vorhersagen\n",
    "\n",
    "        # Cluster-Indizes anpassen, sodass sie global fortlaufend sind\n",
    "        batch_labels[batch_labels != -1] += cluster_id\n",
    "        cluster_id = max(cluster_id, np.max(batch_labels) + 1)  # Update der globalen Cluster-ID\n",
    "\n",
    "        # Bestimme den Start- und End-Index für den aktuellen Batch\n",
    "        start_idx = batch * batch_size\n",
    "        end_idx = min((batch + 1) * batch_size, num_samples)\n",
    "\n",
    "        # Cluster-Zuordnungen aktualisieren\n",
    "        if start_idx < num_samples:\n",
    "            cluster_labels[start_idx:end_idx] = batch_labels[: end_idx - start_idx]\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "\n",
    "# Füge die Clusterzuordnungen zum DataFrame hinzu\n",
    "df_encoded[\"Cluster\"] = cluster_labels\n",
    "\n",
    "print(\"Länge von cluster_labels:\", len(cluster_labels))\n",
    "\n",
    "# Zählen der Cluster (Noise hat den Wert -1)\n",
    "num_clusters = len(np.unique(cluster_labels[cluster_labels != -1]))\n",
    "print(f\"Anzahl der Cluster: {num_clusters}\")\n",
    "\n",
    "# --- Cluster zusammenführen ---\n",
    "# Berechne die Mittelwerte für die Merkmale je Cluster (außer Koordinaten)\n",
    "cluster_means = df_encoded.groupby(\"Cluster\").mean()\n",
    "\n",
    "# Berechne paarweise Distanzen zwischen den Cluster-Mittelwerten\n",
    "cluster_distances = cdist(cluster_means, cluster_means, metric='euclidean')\n",
    "\n",
    "# Definiere eine Schwelle für ähnliche Cluster\n",
    "merge_threshold = 0.42  # Schwelle für Ähnlichkeit (niedrigere Werte führen zu mehr Clusterzusammenführungen)\n",
    "\n",
    "# Identifiziere ähnliche Cluster\n",
    "similar_clusters = np.where(cluster_distances < merge_threshold)\n",
    "\n",
    "# Initialisiere eine Zuordnung für die Cluster (jeder Cluster ist anfangs sich selbst zugeordnet)\n",
    "cluster_mapping = {cluster: cluster for cluster in np.unique(cluster_labels)}\n",
    "\n",
    "# Zusammenführen der ähnlichen Cluster\n",
    "for i, j in zip(*similar_clusters):\n",
    "    if i < j:  # Vermeide doppelte Einträge\n",
    "        cluster_mapping[j] = cluster_mapping[i]  # Cluster j wird mit Cluster i zusammengeführt\n",
    "\n",
    "# Wende die neue Cluster-Zuordnung auf den DataFrame an\n",
    "df_encoded[\"Merged_Cluster\"] = df_encoded[\"Cluster\"].replace(cluster_mapping)\n",
    "\n",
    "# Berechnung des Silhouette-Scores für die zusammengeführten Cluster\n",
    "# Noise-Punkte (mit -1) werden aus den Berechnungen ausgeschlossen\n",
    "valid_labels = df_encoded[\"Merged_Cluster\"] != -1\n",
    "sil_score = silhouette_score(df_encoded[valid_labels], df_encoded.loc[valid_labels, \"Merged_Cluster\"], metric=\"euclidean\")\n",
    "print(f\"Silhouetten-Koeffizient für die gemergten Cluster: {sil_score}\")\n",
    "\n",
    "# Anzahl der zusammengeführten Cluster ausgeben\n",
    "num_merged_clusters = df_encoded[\"Merged_Cluster\"].nunique()\n",
    "print(f\"Anzahl der neuen zusammengeführten Cluster: {num_merged_clusters}\")\n",
    "\n",
    "# Speichern der gemergten Cluster-Daten als CSV\n",
    "df_encoded.to_csv(\"dbscan_merged_clusters.csv\", index=False)\n",
    "print(\"Gemergte Cluster-Daten gespeichert!\")\n",
    "\n",
    "# Speichern der gemergten Cluster als .npy-Datei\n",
    "np.save(\"dbscan_merged_clusters.npy\", df_encoded[\"Merged_Cluster\"].values)\n",
    "print(\"Gemergte Cluster-Daten als .npy gespeichert!\")\n",
    "\n",
    "# ------------------------------------\n",
    "# Davies-Bouldin-Index berechnen auf den gemergten Cluster\n",
    "# ------------------------------------\n",
    "# Clusterzentren der gemergten Cluster\n",
    "merged_cluster_means = df_encoded.groupby(\"Merged_Cluster\").mean()\n",
    "\n",
    "# Berechne paarweise Distanzen zwischen den Mittelwerten der gemergten Cluster\n",
    "merged_cluster_distances = cdist(merged_cluster_means, merged_cluster_means, metric='euclidean')\n",
    "\n",
    "# Berechne den Davies-Bouldin-Index für die gemergten Cluster\n",
    "dbi_score = davies_bouldin_score(df_encoded[merged_cluster_means.columns], df_encoded[\"Merged_Cluster\"])\n",
    "print(f'Davies-Bouldin-Index (gemergte Cluster): {dbi_score}')\n",
    "\n",
    "# ------------------------------------\n",
    "# --- Modell speichern ---\n",
    "# ------------------------------------\n",
    "# Speichern des DBSCAN-Modells\n",
    "joblib.dump(dbscan, 'dbscan_model.pkl')\n",
    "print(\"Das DBSCAN-Modell wurde gespeichert!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f7b99-4f31-416e-a39a-ee21236b7052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2a3a9-5bc4-46f1-a9e6-a5c26d6cf73c",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
